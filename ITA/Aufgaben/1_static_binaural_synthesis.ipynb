{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Header](img/header_1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import sofa\n",
    "HRIR_path = \"hrir/ITA_Artificial_Head_5x5_44100Hz.sofa\"\n",
    "HRIR_dataset = sofa.Database.open(HRIR_path)\n",
    "import helper_functions as hf\n",
    "\n",
    "from IPython.display import Audio\n",
    "from scipy import signal \n",
    "from ipywidgets import Button, IntSlider, Output, Layout\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "\n",
    "# extract the respective positions from the HRIR dataset:\n",
    "source_positions = HRIR_dataset.Source.Position.get_values(system=\"cartesian\")\n",
    "listener_position = np.squeeze(HRIR_dataset.Listener.Position.get_values(system=\"cartesian\"))\n",
    "listener_up = np.squeeze(HRIR_dataset.Listener.Up.get_values(system=\"cartesian\"))\n",
    "listener_view = np.squeeze(HRIR_dataset.Listener.View.get_values(system=\"cartesian\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binauralsynthese\n",
    "\n",
    "Das menschliche Gehör kann Entfernung und Richtung, also Position, einer Quelle bestimmen. Durch \n",
    "Beugungen und Reflexionen am Körper wird das von der Quelle kommende  Signal verändert und \n",
    "trifft in unterschiedlicher Form auf beiden Ohren. Aus der Art der Veränderung und dem Unterschied \n",
    "zwischen den beiden Ohren, wird in unserem auditiven Cortex eine Richtung bestimmt. Ist das \n",
    "Übertragungsverhalten von der Schallquelle zu beiden Ohren bekannt, kann man ein Signal im \n",
    "Vorhinein schon so bearbeiten (filtern), dass es einen Richtungseindruck erzeugt, der es ermöglicht \n",
    "virtuell platzierte Schallquellen im Raum zu platzieren. Dem Hörer kann man also eine beliebige \n",
    "Position der Quelle vortäuschen. Dieses Verfahren wird Binauralesynthese genannt,\n",
    "die auf der Widergabe des binauralen Signals für das linke und rechte Ohr beruht.\n",
    "\n",
    "# 1. Kopfbezogene Außenohrübertragungsfunktionen (HRTFs)\n",
    "\n",
    "Die kopfbezogene Außenohrübertragungsfunktionen beschreibt das bereits erwähnte Übertragungsverhalten von einer Schallquelle, die sich an einer beliebigen Position im Raum befinden kann, zu den Ohren. Sie beinhaltet alle Informationen\n",
    "darüber, wie eine Schallwelle, die von einer Quelle aus einer bestimmten Richtung ausgestrahlt wird, vom \n",
    "Körper der Zuhörenden beeinflusst wird, bevor sie das Trommelfell erreicht. Die Welle wird hier an\n",
    "den Schultern, dem Kopf und der Ohrmuschel gebeugt und reflektiert.\n",
    "Für die Binauralsynthese wird angenommen, dass die HRTF sich als eine Funktion beschreiben lässt,\n",
    "die von der Position der Quelle relativ zum linken und rechten Ohr abhängt. In der Praxis wird die HRTF\n",
    "durch Messungen in konstanten Winkelschritten um den Kopf mit konstantem Abstand ermittelt. Für die Binauralsynthese\n",
    "wird dann die HRTF der Richtung, die sich am nächsten an der angestrebten Quellposition befindet, ausgewählt.\n",
    "\n",
    "Die Richtung der Schallquelle\n",
    "ist beschrieben durch den Elevationswinkel $\\vartheta$, und den Azimuthwinkel $\\varphi$, wobei $\\vartheta$ die Rotation um die interaurale Achse ($y$-Achse), die beide Ohren verbindet, beschreibt und $\\varphi$ die Rotation um die vertikale Achse ($z$-Achse) beschreibt, die \n",
    "von oben durch den Kopf verläuft. Ein positiver Elevationswinkel beschreibt eine Quellposition von oben, ein negativer Winkel eine Quellposition von unten. Ein Azimuthwinkel von $+90^\\circ$ entspricht einer Quellposition links, ein Azimuthwinkel bei $-90^\\circ$ einer Quellposition rechts. Für $0^\\circ$ befindet sich die Quelle vorne, was der $x$-Achse entspricht.\n",
    "\n",
    "<img src=\"img/pti_binaural_synthesis_xyz.png\" width=\"800\" height=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Position einer Schallquelle in einer Umgebung oder einem Raum zu beschreiben werden \n",
    "allgemein Kartesische Koordinaten $(x,y,z)$ verwendet. Es wird hier angenommen, dass der Kopf sich im \n",
    "Ursprung des Koordinatensystems befindet. Die folgende Grafik visualisiert die Position der Quelle relativ \n",
    "zum Kopf. Die Grafik lässt sich zur besseren Erkennung per Maus rotieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Koordinaten hier anpassen\n",
    "x = 1\n",
    "y = 1\n",
    "z = 0\n",
    "\n",
    "source_position = np.array([x, y, z])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(*source_position/np.linalg.norm(source_position), s=30, \n",
    "           label='Source (x={}, y={}, z={})'.format(x, y, z))\n",
    "ax.quiver(*listener_position, *listener_view*0.9, color='C3', label='View vector (x-axis)')\n",
    "ax.quiver(*listener_position, *source_position/np.linalg.norm(source_position)*0.9, color='C0')\n",
    "ax.quiver(*listener_position, *listener_up, color='C2', label='Up vector (z-axis)')\n",
    "ax.quiver(0, 0, 0, 0, 0.9, 0, color='C1', label='y-axis')\n",
    "ax.scatter(*listener_position, s=150, label='Head', color='k')\n",
    "ax.set_xlabel('x in [m]')\n",
    "ax.set_ylabel('y in [m]')\n",
    "ax.set_zlabel('z in [m]')\n",
    "ax.set_xlim([-1, 1])\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.set_zlim([-1, 1])\n",
    "ax.set_xticks([-1, 0, 1])\n",
    "ax.set_yticks([-1, 0, 1])\n",
    "ax.set_zticks([-1, 0, 1])\n",
    "ax.legend(ncol=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1: Berechnung des Elevations und Azimuthwinkels\n",
    "\n",
    "Um die HRTF für die Richtung der Schallquelle zu ermitteln benötigt man nun den Elevations- und Azimuthwinkel.\n",
    "Diese lassen sich berechnen als\n",
    "\n",
    "$$\n",
    "    r = \\sqrt{x^2 + y^2 + z^2} \\\\\n",
    "    \\vartheta = \\arcsin \\left( \\frac{z}{r} \\right) \\\\\n",
    "    \\varphi = \\arctan \\left( \\frac{y}{x} \\right) \\\\ \n",
    "$$\n",
    "\n",
    "**Aufgabe**: Berechnen Sie mit den gegebenen Formeln den Elevations- und Azimuthwinkel für die gegebenen $(x, y, z)$-Koordinaten. Verwenden Sie verschiedene Koordinatenpunkte um ihr Ergebnis zu verifizieren.\n",
    "\n",
    "_Hinweis_:\n",
    "Die benötigten Funktionen und speziellen Operationen sind\n",
    "\n",
    "$\\sqrt{a}$ : `np.sqrt(a)`\n",
    "\n",
    "$a^2$ : `a**2`\n",
    "\n",
    "$\\arctan\\left({a/b}\\right)$ : `np.arctan(a, b)`\n",
    "\n",
    "$\\arcsin\\left({a/b}\\right)$ : `np.arcsin(a/b)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Koordinaten hier anpassen\n",
    "x = 0 \n",
    "y = 1\n",
    "z = 0\n",
    "\n",
    "## Lösung hier eintragen\n",
    "r = 1\n",
    "azimuth = 0\n",
    "elevation = 0\n",
    "## Lösung Ende\n",
    "\n",
    "print(f'Azimuth = {azimuth}')\n",
    "print(f'Elevation = {elevation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "# 2. Die Interaurale Zeitdifferenz und die Interaurale Level-Differenz\n",
    "\n",
    "Die beiden Hauptmechanismen, die vom menschlichen Gehirn verwendet werden, um eine Schallquelle zu lokalisieren,\n",
    "basieren auf der _Interauralen Leveldifferenz (ILD)_  und der _Interauralen Zeitdifferenz (ITD)_. Die ITD beschreibt die\n",
    "zeitliche Verzögerung eines Schallereignisses, der zwischen den beiden Ohren als Resultat der Schallgeschwindigkeit ($c = 343$ m/s) auftritt. Die ILD beschreibt den Lautsstärkeunterschied zwischen beiden Ohren, der durch die Abschattung des durch den Kopf von einem Schallerergbis abgewandten Ohres entsteht. Bei tiefen Frequenzen wertet das menschliche Gehirn vor allem die Zeitdifferenz aus, wobei bei höheren Frequenzen vor allem die Leveldifferenz ausschlaggebend für die Lokalisation ist.\n",
    "\n",
    "## Aufgabe 2.1: Berechnung der Interauralen Zeitdifferenz\n",
    "\n",
    "Schallwellen breiten sich mit einer Geschwindigkeit von ungefähr $c = 343$ m/s aus. Berechnen Sie die Interaurale Zeitdifferenz für eine Schallquelle, die genau von links auf einen Kopf mit einem Durchmesser von $d = 15$ cm trifft. Nehmen Sie an, dass die Schallquelle nicht durch den Kopf wandern kann, sondern auf einer Kreisbahn mit dem Durchmesser des Kopfes um diesen herum wandert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 343 # Schallgeschwindigkeit in m/s\n",
    "d = 15e-2 # Durchmesser in m\n",
    "\n",
    "pi = np.pi\n",
    "\n",
    "# Lösung hier eintragen\n",
    "# Interarale Zeitdifferenz in s\n",
    "ITD = 0 \n",
    "print(f\"ITD = {ITD*1e3} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = HRIR_dataset.Data.SamplingRate.get_values(indices={\"M\":0})\n",
    "from scipy import signal \n",
    "import ipywidgets\n",
    "\n",
    "def get_ITD(HRIR, sampling_rate=44100):\n",
    "    \"\"\"\n",
    "    Get the interaural time difference (ITD) for a specified HRIR.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    HRIR : numpy.ndarray\n",
    "        The HRIR for a single direction.\n",
    "    sampling_rate : integer\n",
    "        The sampling rate of the HRIR.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ITD : double\n",
    "        The interaural time difference (ILD).\n",
    "    \"\"\"\n",
    "    sos = signal.butter(10, 1.5e3, btype='low', output='sos', fs=sampling_rate)\n",
    "    HRIR = signal.sosfilt(sos, HRIR)\n",
    "    n_samples = HRIR.shape[-1]\n",
    "    t = np.arange(0, n_samples)/sampling_rate\n",
    "    corr = signal.correlate(HRIR[1], HRIR[0])\n",
    "    corr_lags = np.arange(-n_samples + 1, n_samples)/sampling_rate\n",
    "    \n",
    "    ITD = np.abs(corr_lags[np.argmax(np.abs(corr))])\n",
    "    \n",
    "    return ITD\n",
    "\n",
    "def get_ILD(HRIR):\n",
    "    \"\"\"\n",
    "    Get the interaural level difference (ILD) for a specified HRIR.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    HRIR : numpy.ndarray\n",
    "        The HRIR for a single direction.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ILD : double\n",
    "        The interaural level difference (ILD).\n",
    "    \"\"\"\n",
    "    energy = np.sum(np.abs(HRIR)**2, axis=-1)\n",
    "    ILD = 10*np.log10(energy[0]/energy[1])\n",
    "    \n",
    "    return ILD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2.2: Ermittlung der ITA aus der HRTF\n",
    "\n",
    "Die Interaurale Zeit- und Leveldifferenz lassen sich wie eingangs beschrieben aus der HRTF bestimmen. Die Interaurale Zeitdiffernz lässt sich hier als die Zeitdifferenz zwischen den Maxima für das linke und das rechte Ohr im Zeitbereich bestimmen (siehe Plot unten links).\n",
    "Die Interaurale Leveldifferenz lässt sich als die Fläche zwischen den Funktionen für das linke und das rechte Ohr berechnen (siehe Plot unten rechts)\n",
    "Verwenden Sie die Slider für den Azimuthwinkel um ihr Ergebnis aus der vorherigen Aufgabe zu überprüfen.\n",
    "## Aufgabe 2.3 Analyse der Interauralen Parameter \n",
    "Verwenden Sie beide Slider um Positionen zu Ermitteln an denen die ILD und die ITD 0 werden. Überlegen Sie, wie dies zu Stande kommt. Welche Auswirkungen hat dies auf die Ortungsfähigkeit des Menschen. Geben Sie eine Deutung."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Platz für Anworten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slider_azimuth = ipywidgets.IntSlider(\n",
    "    value=-35, min=-90, max=90, step=5, description='Azimuth', continuous_update=False)\n",
    "slider_elevation = ipywidgets.IntSlider(\n",
    "    value=0, min=-90, max=90, step=5, description='Elevation', continuous_update=False)\n",
    "\n",
    "interactive_panel = ipywidgets.interact(\n",
    "    hf.plot_HRIR_at_direction,\n",
    "    HRIR_dataset=ipywidgets.fixed(HRIR_dataset),\n",
    "    ILD_function = ipywidgets.fixed(get_ILD),\n",
    "    ITD_function = ipywidgets.fixed(get_ITD),\n",
    "    azimuth=slider_azimuth,\n",
    "    elevation=slider_elevation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# 3. Erstellen der Binauralsynthese\n",
    "\n",
    "Der letzte Aufgabenteil beschäftigt sich mit dem Erstellen der Binauralsynthese. Hierfür wird ein monoaurales Signal $s(t)$, das keinerlei Richtungsinformationen enthält mit der HRTF für die gewünschte Richtung gefaltet. Die Faltung lässt sich mathematisch als \n",
    "\n",
    "$$\n",
    "y_{l}(t, \\vartheta, \\varphi) = \\sum s(t) h_l(t - \\tau, \\vartheta, \\varphi)\\\\\n",
    "y_{r}(t, \\vartheta, \\varphi) = \\sum s(t) h_r(t - \\tau, \\vartheta, \\varphi)\n",
    "$$\n",
    "\n",
    "schreiben, wobei $h_l$ und $h_r$ die HRTF für das linke und rechte Ohr beschreiben, und $y_l$ und $y_r$ das resultierende binaurale Signal für das jeweilige Ohr sind. Es sind also nur die HRTF und das resultierende binaurale Signal verschieden für beide Ohren. Das monoaurale Quellsignal ist unabhängig vom Ohr und nur abhängig von der Schallquelle. Die Faltung und Binauralsynthese sind im Folgenden bereit implementiert und werden nur verwendet.\n",
    "\n",
    "## Aufgabe 3.1: Vergleich von monoauraler und binauraler Wiedergabe\n",
    "**Die folgenden Hörbeispiele erfordern zwingend die Verwendung von Kopfhörern**\n",
    "\n",
    "Im folgenden Widget hören Sie ein Audiosignal, das entweder monoaural, also das selbe Signal auf beiden Ohren, oder binaural wiedergegeben wird. Die Wiedergabemethode lässt sich über die _binaural_ Checkbox umschalten. Vergleichen Sie beide Methoden, verschieben Sie hierfür auch die Quelle mittels des Azimuth sliders. Wie hört sich das monoaurale Signal im Vergleich zum binauralen Signal an?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Platz für Anworten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "def binaural_synthesis_evaluation(instrument='guitar', binaural=False, azi=90, ele=0):\n",
    "    source_signal = hf.read_wav('audio/' + instrument + '.wav')\n",
    "    hrir = hf.get_HRIR_at_direction(HRIR_dataset, azi, ele)\n",
    "    \n",
    "    if not binaural:\n",
    "        out = np.vstack((source_signal, source_signal))\n",
    "    else:\n",
    "        out = np.vstack((\n",
    "            signal.oaconvolve(hrir[1, 80:80+128], source_signal),\n",
    "            signal.oaconvolve(hrir[0, 80:80+128], source_signal)))\n",
    "    return Audio(data=out, rate=44100, autoplay=True)\n",
    "\n",
    "instruments = ['guitar', 'horns', 'vocals']\n",
    "\n",
    "instrument_selector = ipywidgets.Dropdown(\n",
    "    options=instruments,\n",
    "    value='guitar',\n",
    "    description=\"Instrument\"\n",
    ")\n",
    "\n",
    "slider_azimuth = ipywidgets.IntSlider(\n",
    "    value=0, min=-180, max=180, step=5, \n",
    "    description='Azimuth', continuous_update=False,\n",
    "    layout=Layout(height='auto', width='auto'))\n",
    "slider_elevation = ipywidgets.IntSlider(\n",
    "    value=0, min=-90, max=90, step=5,\n",
    "    description='Elevation', continuous_update=False,\n",
    "    layout=Layout(height='auto', width='auto'))\n",
    "\n",
    "ipywidgets.interact(\n",
    "    binaural_synthesis_evaluation,\n",
    "    instrument=instrument_selector,\n",
    "    azi=slider_azimuth,\n",
    "    ele=slider_elevation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3.2: Binauralsynthese mit mehreren Quellen\n",
    "**Die folgenden Hörbeispiele erfordern zwingend die Verwendung von Kopfhörern**\n",
    "\n",
    "Im folgenden Widget können Sie zwei verschiedene Audiosignale binaural wiedergeben. Evaluieren Sie die Binauralsynthese hinsichtlich der Plausibilität der Wiedergabe. Verschieben Sie auch hier wieder auch die Quellen mittels des Azimuth- und Elevationsslider. Es kann hilfreich sein eine Quelle an einer beliebig gewählten Position zu lassen und die andere Position zu variieren. Sie können die Lautstärke der Quellen über den Gain Regler anpassen. Beantworten Sie zur Evaluation die folgenden Fragen:\n",
    "1. Klingt die hier erstellte Binauralsynthese realistisch?\n",
    "2. Welcher Effekt tritt auf, wenn Sie die Schallquelle an Positionen (oder in deren Nähe) bewegen, an denen die ITD und ILD Null werden?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Platz für Anworten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_binaural_signals(\n",
    "        horns_gain, horns_azi, horns_ele, horns_sig,\n",
    "        git_gain, git_azi, git_ele, git_sig, out,\n",
    "    ):\n",
    "    hrir_guitar = hf.get_HRIR_at_direction(HRIR_dataset, git_azi, git_ele)\n",
    "    hrir_horns = hf.get_HRIR_at_direction(HRIR_dataset, horns_azi, horns_ele)\n",
    "    binaural_guitar = np.vstack((\n",
    "        signal.oaconvolve(hrir_guitar[1, 80:80+128], audio_data_guitar),\n",
    "        signal.oaconvolve(hrir_guitar[0, 80:80+128], audio_data_guitar)))\n",
    "\n",
    "    binaural_horns = np.vstack((\n",
    "        signal.oaconvolve(hrir_horns[1, 80:80+128], audio_data_horns),\n",
    "        signal.oaconvolve(hrir_horns[0, 80:80+128], audio_data_horns)))\n",
    "\n",
    "    binaural_mix = 10**(horns_gain/20) * binaural_horns + 10**(git_gain/20) * binaural_guitar\n",
    "    if out is None:\n",
    "        return Audio(data=binaural_mix, rate=44100, autoplay=True)\n",
    "    else:\n",
    "        with out:\n",
    "            audio_out = Audio(data=binaural_mix, rate=44100, autoplay=True, loop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import GridspecLayout\n",
    "from ipywidgets import Button, Layout, jslink, IntText, IntSlider, Output, HBox\n",
    "\n",
    "audio_data_guitar = hf.read_wav('audio/guitar.wav')\n",
    "audio_data_horns = hf.read_wav('audio/vocals.wav')\n",
    "\n",
    "def create_expanded_button(description, button_style):\n",
    "    return Button(\n",
    "        description=description,\n",
    "        button_style=button_style,\n",
    "        layout=Layout(height='auto', width='auto'))\n",
    "\n",
    "slider_azimuth_git = ipywidgets.IntSlider(\n",
    "    value=90, min=-180, max=180, step=5, \n",
    "    description='Azimuth [deg]', continuous_update=False,\n",
    "    layout=Layout(height='auto', width='auto'))\n",
    "slider_elevation_git = ipywidgets.IntSlider(\n",
    "    value=0, min=-90, max=90, step=5,\n",
    "    description='Elevation [deg]', continuous_update=False,\n",
    "    layout=Layout(height='auto', width='auto'))\n",
    "slider_gain_git = ipywidgets.IntSlider(\n",
    "    value=0, min=-50, max=0, step=1,\n",
    "    description='Gain [dB]', continuous_update=False,\n",
    "    layout=Layout(height='auto', width='auto'))\n",
    "\n",
    "\n",
    "slider_azimuth_horns = ipywidgets.IntSlider(\n",
    "    value=0, min=-180, max=180, step=5, \n",
    "    description='Azimuth [deg]', continuous_update=False,\n",
    "    layout=Layout(height='auto', width='auto'))\n",
    "slider_elevation_horns = ipywidgets.IntSlider(\n",
    "    value=0, min=-90, max=90, step=5,\n",
    "    description='Elevation [deg]', continuous_update=False,\n",
    "    layout=Layout(height='auto', width='auto'))\n",
    "slider_gain_horns = ipywidgets.IntSlider(\n",
    "    value=0, min=-50, max=0, step=1,\n",
    "    description='Gain [dB]', continuous_update=False,\n",
    "    layout=Layout(height='auto', width='auto'))\n",
    "\n",
    "\n",
    "grid = GridspecLayout(5, 2, height='200px')\n",
    "grid[0, 0] = create_expanded_button('Source 1', 'success')\n",
    "grid[1, 0] = slider_azimuth_horns\n",
    "grid[2, 0] = slider_elevation_horns\n",
    "grid[3, 0] = slider_gain_horns\n",
    "grid[0, 1] = create_expanded_button('Source 2', 'success')\n",
    "grid[1, 1] = slider_azimuth_git\n",
    "grid[2, 1] = slider_elevation_git\n",
    "grid[3, 1] = slider_gain_git\n",
    "\n",
    "panel = ipywidgets.interact(\n",
    "    play_binaural_signals,\n",
    "    horns_gain=slider_gain_horns,\n",
    "    horns_azi=slider_azimuth_horns,\n",
    "    horns_ele=slider_elevation_horns,\n",
    "    horns_sig=ipywidgets.fixed(audio_data_horns),\n",
    "    git_gain=slider_gain_git,\n",
    "    git_azi=slider_azimuth_git,\n",
    "    git_ele=slider_elevation_git,\n",
    "    git_sig=ipywidgets.fixed(audio_data_guitar),\n",
    "    out=ipywidgets.fixed(None))\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: All audio files have been engineered and recorded by TELEFUNKEN Elektroakustik and are presented for educational and demonstrational purposes only.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
