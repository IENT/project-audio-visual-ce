{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Header](img/header_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Creating a Static Binaural Synthesis\n",
    "---\n",
    "In this lab, the basics of binaural synthesis are introduced in an explorative fashion. The aim is to familiarize with the structure of HRIR datasets, to understand the difference between interaural time difference (ITD) and interaural level difference (ILD), and finally to evaluate a first static binaural synthesis of two monaural input signals.\n",
    "\n",
    "** Important: Always execute all cells in consecutive order, starting at the top of the notebook **\n",
    "\n",
    "## Task 1: Getting familar with HRTF datasets\n",
    "---\n",
    "A HRIR dataset contains the head-related transfer functions, recorded for many discrete directions. The goal  is to simulate two static sound sources from only two directions. The first task is to extract the correct HRIR pairs for both ears from the dataset.\n",
    "\n",
    "The provided dataset `hrir/ITA_Artificial_Head_5x5_44100Hz.sofa` is stored as a SOFA file (Spatially Oriented Format for Acoustics). SOFA enables to store spatially oriented acoustic data like HRIRs. It has been standardized by the Audio Engineering Society (AES) as AES69-2015.\n",
    "\n",
    "### Task 1.1: Loading a HRTF dataset into the workspace\n",
    "---\n",
    "Firstly, a HRIR dataset is loaded into the workspace. You might have a quick look at the documentation of `python-sofa` ([Python-sofa Documentation](https://python-sofa.readthedocs.io/en/latest/)) to get familar with handling sofa files. The HRIR dataset `finishedHRTF_5deg.sofa` is stored in the variable `HRIR_dataset`.\n",
    "\n",
    "*Note: You are not supposed do do any implementation here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sofa\n",
    "HRIR_path = \"hrir/ITA_Artificial_Head_5x5_44100Hz.sofa\"\n",
    "HRIR_dataset = sofa.Database.open(HRIR_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Plotting the source positions of all HRIRs stored in the dataset\n",
    "---\n",
    "In order to get familar with the discrete positions in the dataset, plot the emitter positions `source_positions` of all HRIRs by executing the cell below.\n",
    "\n",
    "The listener's position, the view and up-vector are stored in the variables `listener_position`, `listener_view` and `listener_up`.\n",
    "\n",
    "If you are not already familar with `matplotlib`, which is a library for creating visualizations in Python, you may check out the usage guide: [Matplotlib Usage Guide](https://matplotlib.org/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py)\n",
    "\n",
    "*Note: You are not supposed do do any implementation here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "\n",
    "# extract the respective positions from the HRIR dataset:\n",
    "source_positions = HRIR_dataset.Source.Position.get_values(system=\"cartesian\")\n",
    "listener_position = np.squeeze(HRIR_dataset.Listener.Position.get_values(system=\"cartesian\"))\n",
    "listener_up = np.squeeze(HRIR_dataset.Listener.Up.get_values(system=\"cartesian\"))\n",
    "listener_view = np.squeeze(HRIR_dataset.Listener.View.get_values(system=\"cartesian\"))\n",
    "\n",
    "# plot source positions:\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(source_positions[:, 0], source_positions[:, 1], source_positions[:, 2], s=1)\n",
    "\n",
    "ax.quiver(listener_position[0], listener_position[1], listener_position[2],\n",
    "           listener_view[0], listener_view[1], listener_view[2],\n",
    "           color='red', label='View vector')\n",
    "\n",
    "ax.quiver(listener_position[0], listener_position[1], listener_position[2],\n",
    "           listener_up[0], listener_up[1], listener_up[2],\n",
    "           color='green', label='Up vector')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.set_title('Source Positions')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Interpreting the plot\n",
    "---\n",
    "How is the HRTF dataset oriented and in which direction is the listener looking? In which directions are the ears oriented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down your answer here:\n",
    "\n",
    "# 1) ...\n",
    "# 2) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Interaural level difference (ILD) and the interaural time difference (ITD)\n",
    "---  \n",
    "In the end of this notebook, a scene with two musicians playing in front of the listener will be auralized. Therefore, the HRIRs need to be extracted from two distict directions in which the musicians should be placed virtually. The module `helper_functions` provides some helpful functions for the upcoming tasks.\n",
    "### Task 2.1: Selecting a HRIR from the datataset and print its ILD and ITD\n",
    "1. Select a HRIR from the dataset and print its ILD and ITD. Complete the below cell and use the provided function `hf.get_HRIR_at_direction(HRIR_dataset, azimuth, elevation)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import helper_functions as hf\n",
    "\n",
    "# extract the sampling rate from the dataset:\n",
    "sampling_rate = HRIR_dataset.Data.SamplingRate.get_values(indices={\"M\":0})\n",
    "\n",
    "# define the direction to plot the HRIR for:\n",
    "azimuth = 90\n",
    "elevation = 0\n",
    "\n",
    "###### ! Solution begins here ! ######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Familiarize yourself with the format and shape of the array, the HRIR is stored in. You can use the array method `shape`. For information on numpy arrays, refer to the numpy quickstart guide found at: https://numpy.org/doc/stable/user/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ! Use this cell for the task ! ######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "2. Implement the prepared functions `get_ITD(HRIR)`. For this, you might check section 2.3 in the script. You are not supposed to implement the cross-correlation method. The simplified method based on peak detection is sufficient. You can use the numpy function `np.argmax()` to find the argument (index) for which the input reaches it's maximum. \n",
    "\n",
    "3. Implement the prepared function `get_ILD(HRIR)`. Again, section 2.3 in the script contains more detailled information on the calculation. Assume that for a discrete signal, the integration can be approximated as a summation.\n",
    "\n",
    "4. Finally, the ILD is printed in milliseconds and ITD in decibels. Compare the result with the soultion (`ITD: 0.86 ms, ILD: -14.19 dB`) in order to check if your implementation is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ITD(HRIR, sampling_rate=44100):\n",
    "    \"\"\"\n",
    "    Get the interaural time difference (ITD) for a specified HRIR.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    HRIR : numpy.ndarray\n",
    "        The HRIR for a single direction.\n",
    "    sampling_rate : integer\n",
    "        The sampling rate of the HRIR.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ITD : double\n",
    "        The interaural time difference (ILD).\n",
    "    \"\"\"\n",
    "###### ! Solution begins here ! ######\n",
    "\n",
    "    # Get the time vector and the HRIR for the given direction:\n",
    "    t = np.arange(0,HRIR.shape[-1])/sampling_rate\n",
    "\n",
    "    # Get the respective time instances:\n",
    "    # peak_time_L = ...\n",
    "    # peak_time_R = ...\n",
    "\n",
    "    # Calculate the ITD\n",
    "    # ITD = ...\n",
    "    \n",
    "###### ! Solution ends here ! ######\n",
    "    return ITD\n",
    "\n",
    "def get_ILD(HRIR):\n",
    "    \"\"\"\n",
    "    Get the interaural level difference (ILD) for a specified HRIR.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    HRIR : numpy.ndarray\n",
    "        The HRIR for a single direction.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ILD : double\n",
    "        The interaural level difference (ILD).\n",
    "    \"\"\"\n",
    "###### ! Solution begins here ! ######\n",
    "\n",
    "    # Calculate the integrals for each channel:\n",
    "    # Hint: Assume that the integration can be approximated using\n",
    "    # a summation.\n",
    "\n",
    "    # left =  \n",
    "    # right = \n",
    "    \n",
    "    # Calculate the ILD\n",
    "    \n",
    "    # ILD = \n",
    "    \n",
    "###### ! Solution ends here ! ######\n",
    "    return ILD\n",
    "\n",
    "\n",
    "ITD = get_ITD(HRIR)\n",
    "ILD = get_ILD(HRIR)\n",
    "\n",
    "print('ITD: ' + str(np.round(ITD,5)*1000) + ' ms')\n",
    "print('ILD: ' + str(np.round(ILD,2)) + ' dB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Visualization of the ILD and ITD\n",
    "---\n",
    "Plot the HRIR from the dataset while visualizing its ILD and ITD using the function `hf.plot_HRIR(HRIR, ILD, ITD, sampling_rate)`. Use the previously implemented functions for the calculation of the ITD and ILD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ! Solution begins here ! ######\n",
    "\n",
    "\n",
    "###### ! Solution ends here ! ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Interaural time difference vs. azimuth/elevation\n",
    "---\n",
    "Compare the HRIRs for different azimuth angles using the provided Jupyter widget. Move the slider to look at different azimuth and elevation angles.\n",
    "\n",
    "What do you observe? Please write down in the cell below, how the ITD and ILD are affected by different incident angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "\n",
    "slider_azimuth = ipywidgets.IntSlider(value=0, min=-90, max=90, step=5,\n",
    "                                      description='Azimuth', continuous_update=False)\n",
    "\n",
    "slider_elevation = ipywidgets.IntSlider(value=0, min=-90, max=90, step=5,\n",
    "                                        description='Elevation', continuous_update=False)\n",
    "\n",
    "interactive_panel = ipywidgets.interact(hf.plot_HRIR_at_direction,\n",
    "                                        HRIR_dataset=ipywidgets.fixed(HRIR_dataset),\n",
    "                                        ILD_function = ipywidgets.fixed(get_ILD),\n",
    "                                        ITD_function = ipywidgets.fixed(get_ITD),\n",
    "                                        azimuth=slider_azimuth,\n",
    "                                        elevation=slider_elevation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down your answer here:\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4: Interaural time difference vs. azimuth\n",
    "---\n",
    "In order to summarize the observations, plot the ITD in dependence on the azimuth angle. Complete the code in the cell below and use the the functions from task 2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azimuth_angles = np.arange(-90,90,5)\n",
    "\n",
    "###### ! Solution begins here ! ######\n",
    "\n",
    "ITD = np.zeros(len(azimuth_angles))\n",
    "\n",
    "for idx, azi_angle in enumerate(azimuth_angles):\n",
    "    # ... \n",
    "    \n",
    "    \n",
    "###### ! Solution ends here ! ######\n",
    "\n",
    "# convert to milliseconds:\n",
    "ITD = ITD * 1000\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(azimuth_angles, ITD)\n",
    "ax.set_xlim(-90,90)\n",
    "ax.set_title('ITD vs. Azimuth')\n",
    "ax.set_ylabel('Time [ms]')\n",
    "ax.set_xlabel('Azimuth [deg]')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Convolution and evaluation\n",
    "---\n",
    "In this task, an auralization of a scene with two musicians playing in front of the listener will be implemented.\n",
    "### Task 3.1: Convolution with monaural signal\n",
    "The arrays `audio_data_guitar` and `audio_data_horns` contain monaural recordings of two musicians. You might listen to the files using the audio player widget below.\n",
    "\n",
    "1. Use the function `hf.get_HRIR_at_direction(HRIR_dataset, azimuth, elevation)` to pick two HRIRs from two different directions and store them in a variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(\"audio/guitar.wav\")\n",
    "Audio(\"audio/horns.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Convolve the monaural sources with the respective HRIR. For this use the function `signal.oaconvolve(...)` ([Documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.oaconvolve.html)) and store the results in two arrays. In advance, you need to stack the monaural input to a \"double-mono\" array using `np.vstack(...)` ([Documentation](https://numpy.org/doc/stable/reference/generated/numpy.vstack.html)). Make sure that the convolution is performed along the correct axis of the array.\n",
    "\n",
    "3. Create a mix of the binaural signals for the horn and the guitar by adding them together. Normalize the result using the function `hf.normalize(x)` and store it using the variable `binaural_mixture`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal \n",
    "\n",
    "audio_data_guitar = hf.read_wav('audio/guitar.wav')\n",
    "audio_data_horns = hf.read_wav('audio/horns.wav')\n",
    "\n",
    "###### ! Solution begins here ! ######\n",
    "\n",
    "# ...\n",
    "# binaural_mixture = ...\n",
    "\n",
    "###### ! Solution ends here ! ######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Playback and evaluation of audiblity\n",
    "---\n",
    "Evaluate the resulting audio by listening to it (you have to use headphones). Use the audio player widget below to play back the file.\n",
    "In case you cannot play back audio from the browser, you can download the *.wav files from JupyterHub using the context menu on the left hand side (Right click -> Download).\n",
    "\n",
    "Does the result sound realistic? Name a reason for your observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.write_wav(binaural_mixture, 'output/binaural_mix.wav', 44100)\n",
    "Audio(\"output/binaural_mix.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down your answer here:\n",
    "\n",
    "# 1) ...\n",
    "# 2) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: All audio files have been engineered and recorded by TELEFUNKEN Elektroakustik and are presented for educational and demonstrational purposes only.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
